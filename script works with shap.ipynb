{
 "cells": [
  {
   "attachments": {},
   "cell_type": "markdown",
   "metadata": {},
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "import numpy as np\n",
    "import matplotlib.pyplot as plt\n",
    "from keras.datasets import fashion_mnist\n",
    "from keras.models import Sequential\n",
    "from keras.layers import Conv2D, MaxPooling2D, Flatten, Dense\n",
    "import tensorflow as tf\n",
    "from skimage.color import gray2rgb\n",
    "import shap\n",
    "\n",
    "# Load the FashionMNIST dataset\n",
    "(X_train, y_train), (X_test, y_test) = fashion_mnist.load_data()\n",
    "\n",
    "# Preprocess the data\n",
    "X_train = X_train / 255.0\n",
    "X_test = X_test / 255.0\n",
    "X_train = X_train.reshape(-1, 28, 28, 1)\n",
    "X_test = X_test.reshape(-1, 28, 28, 1)\n"
   ]
  },
  {
   "attachments": {},
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "1   Loading and Preprocessing the Data: The code starts by loading the FashionMNIST dataset and normalizing the pixel values between 0 and 1"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Define the model\n",
    "model = Sequential()\n",
    "model.add(Conv2D(32, (3, 3), activation='relu', input_shape=(28, 28, 1)))\n",
    "model.add(MaxPooling2D((2, 2)))\n",
    "model.add(Conv2D(64, (3, 3), activation='relu'))\n",
    "model.add(Flatten())\n",
    "model.add(Dense(64, activation='relu'))\n",
    "model.add(Dense(10, activation='softmax'))\n",
    "\n",
    "# Compile the model\n",
    "model.compile(optimizer='adam', loss='sparse_categorical_crossentropy', metrics=['accuracy'])\n",
    "\n",
    "# Train the model\n",
    "model.fit(X_train, y_train, epochs=5, batch_size=32, validation_data=(X_test, y_test))\n",
    "\n",
    "class_names = ['T-shirt/top', 'Trouser', 'Pullover', 'Dress', 'Coat',\n",
    "               'Sandal', 'Shirt', 'Sneaker', 'Bag', 'Ankle boot']"
   ]
  },
  {
   "attachments": {},
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "2   Defining and Training the Model: A convolutional neural network (CNN) model is defined using the Sequential API in Keras. The model consists of two convolutional layers followed by max-pooling, a flatten layer, and two dense layers. The model is compiled and trained using the training data."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "\n",
    "# Select two examples\n",
    "example1_idx = np.argmax(y_test == 0)\n",
    "example2_idx = np.argmax(y_test == 1)\n",
    "\n",
    "example1 = X_test[example1_idx]\n",
    "example2 = X_test[example2_idx]\n",
    "\n",
    "# Convert grayscale image to RGB\n",
    "example2_rgb = gray2rgb(example2.reshape(28, 28))\n",
    "\n",
    "# Make predictions for the examples\n",
    "example1_pred = model.predict(example1.reshape(1, 28, 28, 1))\n",
    "example2_pred = model.predict(example2.reshape(1, 28, 28, 1))\n",
    "\n",
    "# Get the predicted classes and probabilities\n",
    "example1_class = np.argmax(example1_pred)\n",
    "example1_prob = example1_pred[0, example1_class]\n",
    "example1_second_prob = np.sort(example1_pred)[0, -2]\n",
    "example1_least_prob = np.sort(example1_pred)[0, -1]\n",
    "\n",
    "example2_class = np.argmax(example2_pred)\n",
    "example2_prob = example2_pred[0, example2_class]\n",
    "example2_second_prob = np.sort(example2_pred)[0, -2]\n",
    "example2_least_prob = np.sort(example2_pred)[0, -1]\n"
   ]
  },
  {
   "attachments": {},
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "3   Selecting Examples: Two examples are selected from the test set, one from class 0 (T-shirt/top) and another from class 1 (Trouser).\n",
    "\n",
    "4   Converting Grayscale Image to RGB: The second example is converted from grayscale to RGB format using the gray2rgb function from skimage.color. This is done to match the input format expected by the SHAP library.\n",
    "\n",
    "5   Making Predictions: The model makes predictions for the two examples using the predict method.\n",
    "\n",
    "6   Getting Predicted Class and Probabilities: The predicted class and probabilities for the true class, most likely class, second most likely class, and least likely class are extracted from the prediction outputs."
   ]
  },
  {
   "attachments": {},
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "\n",
    "print(f\"Example 1: True Class - {class_names[0]}\")\n",
    "print(f\"           Predicted Class - {class_names[example1_class]}\")\n",
    "print(f\"           Probability - {example1_prob:.4f}\")\n",
    "print(f\"           Second Most Likely Probability - {example1_second_prob:.4f}\")\n",
    "print(f\"           Least Likely Probability - {example1_least_prob:.4f}\")\n",
    "print()\n",
    "print(f\"Example 2: True Class - {class_names[1]}\")\n",
    "print(f\"           Predicted Class - {class_names[example2_class]}\")\n",
    "print(f\"           Probability - {example2_prob:.4f}\")\n",
    "print(f\"           Second Most Likely Probability - {example2_second_prob:.4f}\")\n",
    "print(f\"           Least Likely Probability - {example2_least_prob:.4f}\")\n",
    "print()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "\n",
    "# Gradient-based Explanations\n",
    "example1_tensor = tf.convert_to_tensor(example1.reshape(1, 28, 28, 1))\n",
    "with tf.GradientTape() as tape:\n",
    "    tape.watch(example1_tensor)\n",
    "    example1_pred = model(example1_tensor)\n",
    "\n",
    "gradients = tape.gradient(example1_pred, example1_tensor)[0]\n",
    "\n",
    "positive_gradients = np.maximum(0, gradients)\n",
    "negative_gradients = np.minimum(0, gradients)\n",
    "\n",
    "pos_importance_scores = positive_gradients / (np.max(positive_gradients) + 1e-10)\n",
    "neg_importance_scores = negative_gradients / (np.abs(np.min(negative_gradients)) + 1e-10)\n",
    "\n",
    "print(\"Gradient-based Explanations for Example 1:\")\n",
    "print(\"Positive Importance Scores:\")\n",
    "print(pos_importance_scores)\n",
    "print()\n",
    "print(\"Negative Importance Scores:\")\n",
    "print(neg_importance_scores)\n",
    "print()"
   ]
  },
  {
   "attachments": {},
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "\n",
    "7   Gradient-based Explanations: The code calculates the importance scores using the gradients of the model's output with respect to the input for the first example. Positive and negative importance scores are obtained by taking the maximum of 0 and the gradients, and the minimum of 0 and the gradients, respectively.\n",
    "\n",
    "8   Integrated Gradients Explanation: The code uses the SHAP library to compute the SHAP values using the Integrated Gradients method for the second example. A background dataset is randomly sampled from the training set, and a GradientExplainer object is created with the model and the background dataset. The SHAP values are then computed for the example.\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Integrated Gradients Explanation\n",
    "background = X_train[np.random.choice(X_train.shape[0], 100, replace=False)]\n",
    "\n",
    "explainer = shap.GradientExplainer(model, background)\n",
    "shap_values = explainer.shap_values(example2.reshape(1, 28, 28, 1))\n",
    "\n",
    "shap.image_plot(shap_values, -example2.reshape(1, 28, 28, 1))"
   ]
  },
  {
   "attachments": {},
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "9   Plotting SHAP Values: The SHAP values are plotted using the shap.image_plot function to visualize the contributions of each pixel to the prediction.\n",
    "\n",
    "The code provides local explanations for the model's predictions using gradient-based methods, including importance scores based on gradients and SHAP values based on Integrated Gradients. These explanations help in understanding the model's decision-making process and identifying the important features contributing to the predictions."
   ]
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "deep",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.8.16"
  },
  "orig_nbformat": 4
 },
 "nbformat": 4,
 "nbformat_minor": 2
}
