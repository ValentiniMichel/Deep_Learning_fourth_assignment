# Deep_Learning_fourth_assignment

1. Take the model for the FashionMNIST or MNIST data set. Take 2 different examples
from two different classes. Use at least three local explanation methods and explain
reasons they are mapped to the true, the most likely, second most likely, and lest likely
class. Interpret the results. Are the explanations meaningful? Do they differ for diffe-
rent target outputs? What happens if the examples are adversarially attacked (with
a local change of only small parts of the image)? Also try this out experimentally.


2. Use a model which is trained together with a backdoor. Use two different global
explanation methods. Are these capable of detecting/explaining the existence of a
backdoor?
